{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work with restricted Boltzmann machines. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>Exercise 1 (2 points)</H3>\n",
    "\n",
    "Consider the free energy for the RBM:\n",
    "$$\n",
    "F(\\mathbf{v}) = -\\mathbf{b}^T\\mathbf{v} - \\sum_i \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))\\,.\n",
    "$$\n",
    "Derive $\\frac{\\partial F}{\\partial b_j}$, $\\frac{\\partial F}{\\partial c_i}$ and $\\frac{\\partial F}{\\partial w_{ij}}$. Show how you arrive at your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 1 solutions</H3>\n",
    "\n",
    "\n",
    " $\\frac{\\partial F}{\\partial b_j} = \\frac{\\partial -\\mathbf{b}^T\\mathbf{v} - \\sum_i \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))}{\\partial b_j} $ treat the term $ \\sum_i \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))$ as a constant and ignore it gives $\\frac{\\partial -\\mathbf{b}^T\\mathbf{v}}{\\partial b_j} = \\frac{\\partial -(b_1 v_1 + ... + b_j v_j + ... + b_n v_n)}{\\partial b_j} = -v_j$\n",
    " \n",
    "$ \\frac{\\partial F}{\\partial c_j} = \\frac{\\partial -\\mathbf{b}^T\\mathbf{v} - \\sum_i \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))}{\\partial c_j} = \\frac{\\partial \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))}{\\partial c_j} = \\frac{\\partial \\log(x)}{\\partial x} \\frac{\\partial 1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v})}{c_i} = \\frac{1}{x} exp(c_i + w_i^T v) = - \\frac{exp(c_i + w_i^T v)}{1 + exp(c_i + w^t_i v)}$ which is of the same form as $- sigmoid (c_i + w^t_i v)$ and according to \"A Practical Guide to Training Restricted Boltzmann\n",
    "Machines\" page 4, this is the definition of the probability $-p(h_i = 1 | v)$.\n",
    "\n",
    "$\\frac{\\partial F}{\\partial w_{ij}} = \\frac{\\partial -\\mathbf{b}^T\\mathbf{v} - \\sum_i \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))}{\\partial w_{ij}} = \\frac{\\partial \\log(1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v}))}{\\partial w_{ij}} = \\frac{\\partial \\log(x)}{\\partial x} \\frac{\\partial 1 + \\exp(c_i + \\mathbf{w}_i^T\\mathbf{v})}{w_{ij}} = \\frac{1}{x} exp(c_i + w_i^T v) = - \\frac{exp(c_i + w_i^T v)}{1 + exp(c_i + w^t_i v)}$ which is again of the same form as $-sigmoid (c_i + w^t_i v)$ except now the term $w^t_i$ is not treated as a constant so the multiplication with $v$ matters and this is the definition of the probability $-p(h_i = 1 | v)v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>Exercise 2 (8 points)</H3>\n",
    "\n",
    "Restricted Boltzmann machines (RBMs) form the basis for deep belief networks. In this practical exercise you will implement the RBM yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included a dataset digits.mat. This contains a subset of the MNIST digit dataset, discretized to binary values. It contains 10000 digits of size 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('digits.mat')\n",
    "    \n",
    "X = mat['digits'].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the learning algorithm for restricted Boltzmann machines. Try to keep the algorithm as simple as possible (i.e., don’t implement all the various tricks mentioned in the paper). Use *RBM_train* as a starting point. Apply the RBM learning algorithm to the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gibbs(x, W, b, ngibbs=20, temp=1.0):\n",
    "\n",
    "    M = x.size # number of variables\n",
    "    \n",
    "    # perform Gibbs sampling\n",
    "    XM = np.zeros([ngibbs,M])\n",
    "    XM[0] = x\n",
    "    for t in range(1,ngibbs):\n",
    "        for i in range(M):\n",
    "            pi = sigmoid((np.dot(W[i,:],x) + b[i])/temp)\n",
    "            x[i] = np.random.rand() < pi\n",
    "        XM[t,:] = x\n",
    "    \n",
    "    return XM\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def RBM_train(X,N=10,T=10):\n",
    "    # RBM_TRAIN trains a restricted Boltzmann machine on M x nexamples input\n",
    "    # data X with N hidden units for T time steps.\n",
    "    #\n",
    "    # Returns N x M weights W, M x 1 visible unit bias b and N x 1 hidden unit bias c\n",
    "\n",
    "    M = X.shape[0]\n",
    "    W = 10**-1*np.random.normal(size=[N,M])\n",
    "    b = 10**-1*np.random.normal(size=M)\n",
    "    c = np.zeros(N)\n",
    "    \n",
    "    # iterate over examples (iterating over minibatches would be more elegant!)\n",
    "    for idx in range(T):\n",
    "\n",
    "        prm = np.random.permutation(X.shape[1])\n",
    "        for i in prm:\n",
    "            \n",
    "            # Implementation of the CD-1 algorithm\n",
    "            gibbs(X, W, b)\n",
    "            \n",
    "\n",
    "    return W,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, b, c = RBM_train(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rows of W as receptive fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now generate fantasies. RBMs are so-called generative models. This means that they can be used to generate visible states. This can be done by running Gibbs sampling for T≫1 steps (instead of just one step in CD-1) from a random initial state of the hidden units. Implement a function *RBM_test(W, b, c, T)* which implements this procedure and visualizes the state of the visible units during Gibbs sampling. Add five different fantasies to your pdf. Reflect on the generated fantasies and how this relates to the learnt parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
