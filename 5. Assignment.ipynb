{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work on the autoencoder. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 1 (7 points)</H3>\n",
    "\n",
    "We reuse the code you implemented in the MLP assignment. We will use the code to train an autoencoder and reconstruct face images taken from the Yale face database (http://vismod.media.mit.edu/vismod/classes/mas622-00/datasets). For illustration we test reconstruction performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function; returns function value and gradient\n",
    "    \"\"\"\n",
    "\n",
    "    fx = 1.0 / (1 + np.exp(-x))\n",
    "    gradx = fx * (1 - fx)\n",
    "\n",
    "    return fx, gradx\n",
    "\n",
    "def error(f_a_3,T):\n",
    "    \"\"\"\n",
    "    Computes squared error divided by number of trials\n",
    "    \n",
    "    Input:\n",
    "    f_a_3 : MLP output states\n",
    "    T   : noutput x ntrials targets\n",
    "\n",
    "    Output:\n",
    "    E_w        : squared error\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    ntrials = T.shape[1]\n",
    "\n",
    "    E_w = 1.0 / (2 * ntrials) * np.sum(np.sum((f_a_3 - T) ** 2))\n",
    "   \n",
    "    return E_w\n",
    "\n",
    "def forwardprop(W_1, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs forward propagation\n",
    "    \n",
    "    Input:\n",
    "    W_1 : nhidden x ninput input-to-hidden weight matrix\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # You should now implement the forward propagation function. Your\n",
    "    # implementation should compute and return the outputs of the second and\n",
    "    # third layer units as well as their gradients.\n",
    "\n",
    "    # First, compute the inputs of the second layer units (i.e. a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    a_2 = np.dot(W_1, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_2, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the second\n",
    "    # layer units (i.e. f_a_2) and their gradients (i.e. grad_f_a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    f_a_2, grad_f_a_2 = sigmoid(a_2) \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Then, compute the inputs of the third layer units (i.e. a_3). Write your\n",
    "    # code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    a_3 = np.dot(W_2, f_a_2)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_3, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the third layer\n",
    "    # units (i.e. f_a_3) and their gradients (i.e. grad_f_a_3). Write your code\n",
    "    # below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    f_a_3, grad_f_a_3 = sigmoid(a_3)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    return f_a_2, f_a_3, grad_f_a_2, grad_f_a_3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct a dataset from the face database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backprop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-98371c6ca212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[1;31m# and grad_E_w_2). Write your code below:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[1;33m[\u001b[0m\u001b[0mgrad_E_w_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_E_w_2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_a_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_a_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_f_a_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_f_a_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'backprop' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import scipy.stats as stat\n",
    "\n",
    "n = len(os.listdir(\"yalefaces\"))\n",
    "\n",
    "maxsz = [32, 32]\n",
    "\n",
    "X = []\n",
    "i=0\n",
    "for file in os.listdir(\"yalefaces\"):\n",
    "    im = Image.open(os.getcwd() + '/yalefaces/' + file)\n",
    "    im.thumbnail(maxsz, Image.ANTIALIAS)\n",
    "    data = np.asarray(im)\n",
    "    if i==0:\n",
    "        sz = data.shape\n",
    "    X.append(np.ndarray.flatten(data))\n",
    "    i+=1\n",
    "\n",
    "# convert to numpy array\n",
    "X = np.array(X).astype('float32')\n",
    "\n",
    "# zscore operation\n",
    "mu = np.mean(X,0)\n",
    "X -= mu\n",
    "sigma = np.std(X,0)\n",
    "X /= sigma\n",
    "\n",
    "# make suitable for training\n",
    "X = X.transpose()\n",
    "\n",
    "\n",
    "nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "ninput = X.shape[0]\n",
    "noutput = X.shape[0]\n",
    "nhidden = 30\n",
    "\n",
    "# initialize weights\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_1 = np.random.uniform(-r, r, [nhidden,ninput])\n",
    "\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_2 = np.random.uniform(-r, r, [noutput,nhidden])\n",
    "\n",
    "# keep track of errors\n",
    "train_error = np.zeros([nepochs+1,1])\n",
    "test_error = np.zeros([nepochs+1,1])\n",
    "\n",
    "for epoch in xrange(0,nepochs):\n",
    "\n",
    "    # First, use the forward propagation function that you have implemented\n",
    "    # (i.e. forwardprop) to compute the outputs of the second and third layer\n",
    "    # units (i.e. f_a_2 and f_a_3) as well as their gradients (i.e. grad_f_a_2\n",
    "    # and grad_f_a_3). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [f_a_2, f_a_3, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # compute error\n",
    "    train_error[epoch] = error(f_a_3, X)\n",
    "    test_error[epoch] = error(forwardprop(W_1, W_2, X)[1], X)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "         print('Iteration: ' + str(epoch+1) + ' / ' + str(nepochs) + '; Train error: ' \n",
    "               + str(train_error[epoch])) + '; Test error: ' + str(test_error[epoch])\n",
    " \n",
    "    # Once you have computed f_a_2, f_a_3, grad_f_a_2 and grad_f_a_3, use them\n",
    "    # with the back propagation function that you have implemented (i.e.\n",
    "    # backprop) to compute the gradients of the error function (i.e. grad_E_w_1\n",
    "    # and grad_E_w_2). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [grad_E_w_1, grad_E_w_2] = backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, X, W_2, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "             \n",
    "    W_1 = W_1 - learning_rate * grad_E_w_1                                 \n",
    "    W_2 = W_2 - learning_rate * grad_E_w_2                                                                            \n",
    "    \n",
    "# get error after the last update\n",
    "train_error[-1] = error(forwardprop(W_1, W_2, X)[1], X)\n",
    "test_error[-1] = error(forwardprop(W_1, W_2, X)[1], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the linear autoencoder in the undercomplete setting (e.g. nhidden=30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first three original faces and their reconstructions by the autoencoder. Make sure to take the previously applied zscoring operation into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[f_a_2, X_hat, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(np.reshape(X.T[0]*sigma + mu, sz))\n",
    "plt.title('Original 0')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[0]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 0')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X.T[1]*sigma +mu, sz))\n",
    "plt.title('Original 1')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[1]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 1')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X.T[2]*sigma + mu, sz))\n",
    "plt.title('Original 2')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[2]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 2')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (3 points)</H3>\n",
    "\n",
    "Plot the rows and columns of the weight matrices *W1* and *W2* reshaped to the image width and height to check the receptive and projective field structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,30):\n",
    "    plt.subplot(5,6,i)\n",
    "    plt.imshow(np.reshape(W_1[i], sz))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print W_2.size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
