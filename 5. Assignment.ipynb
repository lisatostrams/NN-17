{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work on the autoencoder. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 1 (7 points)</H3>\n",
    "\n",
    "We reuse the code you implemented in the MLP assignment. We will use the code to train an autoencoder and reconstruct face images taken from the Yale face database (http://vismod.media.mit.edu/vismod/classes/mas622-00/datasets). For illustration we test reconstruction performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function; returns function value and gradient\n",
    "    \"\"\"\n",
    "\n",
    "    fx = 1.0 / (1 + np.exp(-x))\n",
    "    gradx = fx * (1 - fx)\n",
    "\n",
    "    return fx, gradx\n",
    "\n",
    "def error(f_a_3,T):\n",
    "    \"\"\"\n",
    "    Computes squared error divided by number of trials\n",
    "    \n",
    "    Input:\n",
    "    f_a_3 : MLP output states\n",
    "    T   : noutput x ntrials targets\n",
    "\n",
    "    Output:\n",
    "    E_w        : squared error\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    ntrials = T.shape[1]\n",
    "\n",
    "    E_w = 1.0 / (2 * ntrials) * np.sum(np.sum((f_a_3 - T) ** 2))\n",
    "   \n",
    "    return E_w\n",
    "\n",
    "def forwardprop(W_1, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs forward propagation\n",
    "    \n",
    "    Input:\n",
    "    W_1 : nhidden x ninput input-to-hidden weight matrix\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # You should now implement the forward propagation function. Your\n",
    "    # implementation should compute and return the outputs of the second and\n",
    "    # third layer units as well as their gradients.\n",
    "\n",
    "    # First, compute the inputs of the second layer units (i.e. a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    a_2 = np.dot(W_1, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_2, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the second\n",
    "    # layer units (i.e. f_a_2) and their gradients (i.e. grad_f_a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    f_a_2, grad_f_a_2 = sigmoid(a_2) \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Then, compute the inputs of the third layer units (i.e. a_3). Write your\n",
    "    # code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    a_3 = np.dot(W_2, f_a_2)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_3, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the third layer\n",
    "    # units (i.e. f_a_3) and their gradients (i.e. grad_f_a_3). Write your code\n",
    "    # below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    f_a_3, grad_f_a_3 = sigmoid(a_3)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    return f_a_2, f_a_3, grad_f_a_2, grad_f_a_3\n",
    "\n",
    "\n",
    "def backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, T, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs backpropagation step\n",
    "    \n",
    "    Input:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    T   : noutput x ntrials targets\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    grad_E_w_1 : ntrials x 1 gradient of the error w.r.t W_1\n",
    "    grad_E_w_2 : ntrials x 1 gradient of the error w.r.t W_2\n",
    "    \"\"\"\n",
    "        \n",
    "    # You should now implement the back propagation function. Your\n",
    "    # implementation should compute and return the gradients of the error\n",
    "    # function.\n",
    "\n",
    "    # First, compute the errors of the second and third layer units (i.e.\n",
    "    # delta_2 and delta_3). Write you code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    delta_3 = (f_a_3 - T) * grad_f_a_3\n",
    "                   \n",
    "    delta_2 = grad_f_a_2 *(np.dot(W_2.T, delta_3))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed delta_2 and delta_3, use them to compute the\n",
    "    # gradients of the error function (i.e. grad_E_w_1 and grad_E_w_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    " \n",
    "    # Add your solution here.\n",
    "    grad_E_w_2 = np.dot(delta_3, f_a_2.T)\n",
    "    \n",
    "    grad_E_w_1 = np.dot(delta_2, X.T)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    return grad_E_w_1, grad_E_w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct a dataset from the face database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 / 2000; Train error: [ 328.20510348]; Test error: [ 328.20510348]\n",
      "Iteration: 200 / 2000; Train error: [ 315.89187173]; Test error: [ 315.89187173]\n",
      "Iteration: 300 / 2000; Train error: [ 309.46551002]; Test error: [ 309.46551002]\n",
      "Iteration: 400 / 2000; Train error: [ 305.43067042]; Test error: [ 305.43067042]\n",
      "Iteration: 500 / 2000; Train error: [ 302.60831783]; Test error: [ 302.60831783]\n",
      "Iteration: 600 / 2000; Train error: [ 300.5263269]; Test error: [ 300.5263269]\n",
      "Iteration: 700 / 2000; Train error: [ 298.92119474]; Test error: [ 298.92119474]\n",
      "Iteration: 800 / 2000; Train error: [ 297.61227376]; Test error: [ 297.61227376]\n",
      "Iteration: 900 / 2000; Train error: [ 296.47591597]; Test error: [ 296.47591597]\n",
      "Iteration: 1000 / 2000; Train error: [ 295.53256677]; Test error: [ 295.53256677]\n",
      "Iteration: 1100 / 2000; Train error: [ 294.71593538]; Test error: [ 294.71593538]\n",
      "Iteration: 1200 / 2000; Train error: [ 294.01683936]; Test error: [ 294.01683936]\n",
      "Iteration: 1300 / 2000; Train error: [ 293.35982515]; Test error: [ 293.35982515]\n",
      "Iteration: 1400 / 2000; Train error: [ 292.80844819]; Test error: [ 292.80844819]\n",
      "Iteration: 1500 / 2000; Train error: [ 292.31622239]; Test error: [ 292.31622239]\n",
      "Iteration: 1600 / 2000; Train error: [ 291.86955963]; Test error: [ 291.86955963]\n",
      "Iteration: 1700 / 2000; Train error: [ 291.4544109]; Test error: [ 291.4544109]\n",
      "Iteration: 1800 / 2000; Train error: [ 291.06148121]; Test error: [ 291.06148121]\n",
      "Iteration: 1900 / 2000; Train error: [ 290.7084594]; Test error: [ 290.7084594]\n",
      "Iteration: 2000 / 2000; Train error: [ 290.36987294]; Test error: [ 290.36987294]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import scipy.stats as stat\n",
    "\n",
    "n = len(os.listdir(\"yalefaces\"))\n",
    "\n",
    "maxsz = [32, 32]\n",
    "\n",
    "X = []\n",
    "i=0\n",
    "for file in os.listdir(\"yalefaces\"):\n",
    "    im = Image.open(os.getcwd() + '/yalefaces/' + file)\n",
    "    im.thumbnail(maxsz, Image.ANTIALIAS)\n",
    "    data = np.asarray(im)\n",
    "    if i==0:\n",
    "        sz = data.shape\n",
    "    X.append(np.ndarray.flatten(data))\n",
    "    i+=1\n",
    "\n",
    "# convert to numpy array\n",
    "X = np.array(X).astype('float32')\n",
    "\n",
    "# zscore operation\n",
    "mu = np.mean(X,0)\n",
    "X -= mu\n",
    "sigma = np.std(X,0)\n",
    "X /= sigma\n",
    "\n",
    "# make suitable for training\n",
    "X = X.transpose()\n",
    "\n",
    "\n",
    "nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "ninput = X.shape[0]\n",
    "noutput = X.shape[0]\n",
    "nhidden = 30\n",
    "\n",
    "# initialize weights\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_1 = np.random.uniform(-r, r, [nhidden,ninput])\n",
    "\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_2 = np.random.uniform(-r, r, [noutput,nhidden])\n",
    "\n",
    "# keep track of errors\n",
    "train_error = np.zeros([nepochs+1,1])\n",
    "test_error = np.zeros([nepochs+1,1])\n",
    "\n",
    "for epoch in xrange(0,nepochs):\n",
    "\n",
    "    # First, use the forward propagation function that you have implemented\n",
    "    # (i.e. forwardprop) to compute the outputs of the second and third layer\n",
    "    # units (i.e. f_a_2 and f_a_3) as well as their gradients (i.e. grad_f_a_2\n",
    "    # and grad_f_a_3). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [f_a_2, f_a_3, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # compute error\n",
    "    train_error[epoch] = error(f_a_3, X)\n",
    "    test_error[epoch] = error(forwardprop(W_1, W_2, X)[1], X)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "         print('Iteration: ' + str(epoch+1) + ' / ' + str(nepochs) + '; Train error: ' \n",
    "               + str(train_error[epoch])) + '; Test error: ' + str(test_error[epoch])\n",
    " \n",
    "    # Once you have computed f_a_2, f_a_3, grad_f_a_2 and grad_f_a_3, use them\n",
    "    # with the back propagation function that you have implemented (i.e.\n",
    "    # backprop) to compute the gradients of the error function (i.e. grad_E_w_1\n",
    "    # and grad_E_w_2). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [grad_E_w_1, grad_E_w_2] = backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, X, W_2, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "             \n",
    "    W_1 = W_1 - learning_rate * grad_E_w_1                                 \n",
    "    W_2 = W_2 - learning_rate * grad_E_w_2                                                                            \n",
    "    \n",
    "# get error after the last update\n",
    "train_error[-1] = error(forwardprop(W_1, W_2, X)[1], X)\n",
    "test_error[-1] = error(forwardprop(W_1, W_2, X)[1], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the linear autoencoder in the undercomplete setting (e.g. nhidden=30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first three original faces and their reconstructions by the autoencoder. Make sure to take the previously applied zscoring operation into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[f_a_2, X_hat, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(np.reshape(X.T[0]*sigma + mu, sz))\n",
    "plt.title('Original 0')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[0]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 0')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X.T[1]*sigma +mu, sz))\n",
    "plt.title('Original 1')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[1]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 1')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X.T[2]*sigma + mu, sz))\n",
    "plt.title('Original 2')\n",
    "plt.show()\n",
    "plt.imshow(np.reshape(X_hat.T[2]*sigma + mu, sz))\n",
    "plt.title('Reconstructed 2')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (3 points)</H3>\n",
    "\n",
    "Plot the rows and columns of the weight matrices *W1* and *W2* reshaped to the image width and height to check the receptive and projective field structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,30):\n",
    "    plt.subplot(5,6,i)\n",
    "    plt.imshow(np.reshape(W_1[i], sz))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print W_2.size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
