{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this practical assignment you will learn how to train a recurrent neural network for language generation in chainer. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide most of the code needed for this assignment. The assignment is based on https://github.com/pfnet/chainer/blob/master/examples/ptb/train_ptb.py. We start by defining a neural network for language generation. Note that this network combines a word embedding layer (mapping word indices to dense vectors) with an LSTM layer (capturing word dynamics) and a linear output layer (yielding the word probabilities via a Classifier object's softmax layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "# Definition of a recurrent net for language modeling\n",
    "class RNNForLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNForLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h2 = self.l2(F.dropout(h1, train=self.train))\n",
    "        y = self.l3(F.dropout(h2, train=self.train))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a custom iterator to deal with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset iterator to create a batch of sequences at different positions.\n",
    "# This iterator returns a pair of current words and the next words. Each\n",
    "# example is a part of sequences starting from the different offsets\n",
    "# equally spaced within the whole sequence.\n",
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of ``__next__``.\n",
    "        self.iteration = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define an updater which runs backpropagation through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom updater for truncated BackProp Through Time (BPTT)\n",
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        # bprop_len = Number of words in each mini-batch (= length of truncated BPTT)\n",
    "        \n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "        \n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Penn tree bank text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Penn Tree Bank long word sequence dataset\n",
    "train, val, test = chainer.datasets.get_ptb_words()\n",
    "n_vocab = max(train) + 1  # train is just an array of integers\n",
    "\n",
    "# Reduce to first 1000 words to speed up\n",
    "train = train[:1000]\n",
    "val = val[:1000]\n",
    "test = test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the change in perplexity to get a measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Routine to rewrite the result dictionary of LogReport to add perplexity values\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       perplexity  val_perplexity\n",
      "\u001b[J3           10082.6     5874.5          \n",
      "\u001b[J7           5634.72     2973.87         \n",
      "\u001b[J10          3477.29     4612.93         \n",
      "\u001b[J14          4042.22     2818.94         \n",
      "\u001b[J17          2800.59     2933.97         \n",
      "\u001b[J21          2457.97     2898.76         \n",
      "\u001b[J24          2775.19     2753.37         \n",
      "\u001b[J28          2044.07     1908.38         \n",
      "\u001b[J31          1701.38     2302.01         \n",
      "\u001b[J35          1467.23     2594.21         \n",
      "\u001b[J38          1692.55     2293.95         \n",
      "\u001b[J42          1317.15     1609.11         \n",
      "\u001b[J45          972.369     38997.5         \n",
      "\u001b[J49          13027       2366.29         \n",
      "\u001b[J52          1132.72     1773.11         \n",
      "\u001b[J56          860.635     10363.9         \n",
      "\u001b[J59          5988.28     2214.65         \n",
      "\u001b[J63          823.516     2193.91         \n",
      "\u001b[J66          891         2429.45         \n",
      "\u001b[J70          841.851     1776.46         \n",
      "\u001b[J73          694.775     4651.82         \n",
      "\u001b[J77          1156.82     2353.07         \n",
      "\u001b[J80          808.364     2111.47         \n",
      "\u001b[J84          604.463     2180.09         \n",
      "\u001b[J87          648.483     3071.28         \n",
      "\u001b[J91          826.578     2238.75         \n",
      "\u001b[J94          829.088     2459.16         \n",
      "\u001b[J98          575.512     1970.74         \n",
      "\u001b[J101         639.202     2612.13         \n"
     ]
    }
   ],
   "source": [
    "# Define iterators\n",
    "train_iter = ParallelSequentialIterator(train, batch_size=100)\n",
    "val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "test_iter = ParallelSequentialIterator(test, 1, repeat=False)\n",
    "\n",
    "# Prepare an RNNLM model with 100 LSTM units\n",
    "rnn = RNNForLM(n_vocab, 100)\n",
    "model = L.Classifier(rnn)\n",
    "\n",
    "# Set up an optimizer\n",
    "optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(5))\n",
    "\n",
    "# Set up a trainer\n",
    "updater = BPTTUpdater(train_iter, optimizer, bprop_len=35, device=-1)\n",
    "trainer = training.Trainer(updater, (100, 'epoch'), out='result')\n",
    "\n",
    "# Model with shared params and distinct states on which we evaluate the validation data\n",
    "eval_model = model.copy()\n",
    "eval_rnn = eval_model.predictor\n",
    "eval_rnn.train = False\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    val_iter, eval_model, device=-1,\n",
    "    # Reset the RNN state at the beginning of each evaluation\n",
    "    eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "# Add things to report\n",
    "trainer.extend(extensions.LogReport(postprocess=compute_perplexity,\n",
    "                                    trigger=(1, 'epoch')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'perplexity', 'val_perplexity']\n",
    "), trigger=(1, 'epoch'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the trained model for language generation. We will use converter from index notation to word identity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_idx = chainer.datasets.get_ptb_words_vocabulary()\n",
    "\n",
    "# create reverse vocabulary\n",
    "idx_to_word = {}\n",
    "for k in word_to_idx.keys():\n",
    "    idx_to_word[word_to_idx[k]] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (3 points)\n",
    "\n",
    "Now write code to generate words using the predictor of the trained model. Ensure to set the *train* field of the predictor to *False*. This ensures that the dropout layer (as used in the model) works correctly. Generate new words by probabilistically selecting a new word using the predictor. Note that the outputs of the predictor need to be converted first to probabilities using a ```softmax``` function. To probabilistically select a new word, use ```numpy.random.multinomial```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 points)\n",
    "\n",
    "Use your code to generate a list of 30 words. Does it resemble written text? Why (not)? Explain your answer. What is needed to further improve text generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add words and explanations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training the recurrent neural network, you implicitly learnt a word embedding. Use the code below to get the dense 100D word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = np.unique(train)\n",
    "ann = model.predictor\n",
    "\n",
    "X = ann.embed(np.array(words,dtype='int32')).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (5 points)\n",
    "\n",
    "Now run a PCA analysis such that each word is mapped to a 2D representational space. Use the scikit-learn library to achieve this (http://scikit-learn.org). Also add the locations of the following four concepts explicitly to your scatter plot using different colours: *smokers*, *cancer*, *portfolio*, *market*. Do the representations in this 2D space make sense? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code and explanations here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
