{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this practical assignment you will learn how to train a recurrent neural network for language generation in chainer. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide most of the code needed for this assignment. The assignment is based on https://github.com/pfnet/chainer/blob/master/examples/ptb/train_ptb.py. We start by defining a neural network for language generation. Note that this network combines a word embedding layer (mapping word indices to dense vectors) with an LSTM layer (capturing word dynamics) and a linear output layer (yielding the word probabilities via a Classifier object's softmax layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "# Definition of a recurrent net for language modeling\n",
    "class RNNForLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNForLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h2 = self.l2(F.dropout(h1, train=self.train))\n",
    "        y = self.l3(F.dropout(h2, train=self.train))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a custom iterator to deal with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset iterator to create a batch of sequences at different positions.\n",
    "# This iterator returns a pair of current words and the next words. Each\n",
    "# example is a part of sequences starting from the different offsets\n",
    "# equally spaced within the whole sequence.\n",
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of ``__next__``.\n",
    "        self.iteration = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define an updater which runs backpropagation through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom updater for truncated BackProp Through Time (BPTT)\n",
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        # bprop_len = Number of words in each mini-batch (= length of truncated BPTT)\n",
    "        \n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "        \n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Penn tree bank text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Penn Tree Bank long word sequence dataset\n",
    "train, val, test = chainer.datasets.get_ptb_words()\n",
    "n_vocab = max(train) + 1  # train is just an array of integers\n",
    "\n",
    "# Reduce to first 1000 words to speed up\n",
    "train = train[:1000]\n",
    "val = val[:1000]\n",
    "test = test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the change in perplexity to get a measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Routine to rewrite the result dictionary of LogReport to add perplexity values\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       perplexity  val_perplexity\n",
      "3           10047.2     5843.44         \n",
      "7           5615.32     2996.96         \n",
      "10          3368.28     4684.92         \n",
      "14          4171.62     2883.17         \n",
      "17          3294.54     3153.98         \n",
      "21          2536.81     2510.58         \n",
      "24          2409.47     2834.33         \n",
      "28          2051.36     1981.68         \n",
      "31          1647.49     2521.9          \n",
      "35          1709.71     2298.31         \n",
      "38          1553.5      2210.9          \n",
      "42          1239.9      2249.52         \n",
      "45          1806.64     2360.3          \n",
      "49          1000.64     2015.17         \n",
      "52          1067.15     6237.5          \n",
      "56          1991.87     2073.07         \n",
      "59          938.669     4173.4          \n",
      "63          1685.05     2001.06         \n",
      "66          1164.85     2297.18         \n",
      "70          653.808     1834.88         \n",
      "73          968.005     2513.62         \n",
      "77          924.807     2145.96         \n",
      "80          888.545     2135.98         \n",
      "84          611.324     2103.14         \n",
      "87          816.9       2010.92         \n",
      "91          514.71      2561.93         \n",
      "94          658.564     1760.24         \n",
      "98          557.717     2727.6          \n",
      "101         652.662     2587.45         \n"
     ]
    }
   ],
   "source": [
    "# Define iterators\n",
    "train_iter = ParallelSequentialIterator(train, batch_size=100)\n",
    "val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "test_iter = ParallelSequentialIterator(test, 1, repeat=False)\n",
    "\n",
    "# Prepare an RNNLM model with 100 LSTM units\n",
    "rnn = RNNForLM(n_vocab, 100)\n",
    "model = L.Classifier(rnn)\n",
    "\n",
    "# Set up an optimizer\n",
    "optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(5))\n",
    "\n",
    "# Set up a trainer\n",
    "updater = BPTTUpdater(train_iter, optimizer, bprop_len=35, device=-1)\n",
    "trainer = training.Trainer(updater, (100, 'epoch'), out='result')\n",
    "\n",
    "# Model with shared params and distinct states on which we evaluate the validation data\n",
    "eval_model = model.copy()\n",
    "eval_rnn = eval_model.predictor\n",
    "eval_rnn.train = False\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    val_iter, eval_model, device=-1,\n",
    "    # Reset the RNN state at the beginning of each evaluation\n",
    "    eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "# Add things to report\n",
    "trainer.extend(extensions.LogReport(postprocess=compute_perplexity,\n",
    "                                    trigger=(1, 'epoch')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'perplexity', 'val_perplexity']\n",
    "), trigger=(1, 'epoch'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the trained model for language generation. We will use converter from index notation to word identity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_idx = chainer.datasets.get_ptb_words_vocabulary()\n",
    "\n",
    "# create reverse vocabulary\n",
    "idx_to_word = {}\n",
    "for k in word_to_idx.keys():\n",
    "    idx_to_word[word_to_idx[k]] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (3 points)\n",
    "\n",
    "Now write code to generate words using the predictor of the trained model. Ensure to set the *train* field of the predictor to *False*. This ensures that the dropout layer (as used in the model) works correctly. Generate new words by probabilistically selecting a new word using the predictor. Note that the outputs of the predictor need to be converted first to probabilities using a ```softmax``` function. To probabilistically select a new word, use ```numpy.random.multinomial```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old years deukmejian filters one-hour of recessions reinforcing handle to a any tale factory <unk> impossible up incomplete block by byrd intermediate hottest kent old estimate city said fundamentals caused\n"
     ]
    }
   ],
   "source": [
    "pred = model.predictor\n",
    "pred.train = False\n",
    "\n",
    "random_words = []\n",
    "\n",
    "prediction = F.softmax(pred(train[:100]).data).data #cant use all training data?\n",
    "#how to use predictor?\n",
    "\n",
    "sample = np.random.multinomial(1, prediction[0,:], size=30)\n",
    "#sample from one row of predictor output?\n",
    "\n",
    "for i in range(30):\n",
    "    idx = list(sample[i,:] == 1).index(True)\n",
    "    random_words.append(idx_to_word[idx])\n",
    "\n",
    "print(' '.join(random_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 points)\n",
    "\n",
    "Use your code to generate a list of 30 words. Does it resemble written text? Why (not)? Explain your answer. What is needed to further improve text generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add words and explanations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training the recurrent neural network, you implicitly learnt a word embedding. Use the code below to get the dense 100D word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = np.unique(train)\n",
    "ann = model.predictor\n",
    "\n",
    "X = ann.embed(np.array(words,dtype='int32')).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (5 points)\n",
    "\n",
    "Now run a PCA analysis such that each word is mapped to a 2D representational space. Use the scikit-learn library to achieve this (http://scikit-learn.org). Also add the locations of the following four concepts explicitly to your scatter plot using different colours: *smokers*, *cancer*, *portfolio*, *market*. Do the representations in this 2D space make sense? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code and explanations here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
