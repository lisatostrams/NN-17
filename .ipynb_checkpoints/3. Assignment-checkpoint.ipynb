{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this assignment you will develop your first neural network and gain more understanding on their mathematical background. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>Exercise 1 (3 points)</H3>\n",
    "\n",
    "We now move on to the first neural network exercises.\n",
    "1. The threshold activation function\n",
    "\\begin{equation}\n",
    "f(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\textrm{if $x\\geq 0$}\\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "is equivalent to the binary threshold unit of McCulloch and Pitts, who showed that logical operations can be done with neural networks. Show how to build a logical AND gate with the perceptron. I.e. write down the truth table and choose weights and a bias term which reproduce the correct output.\n",
    "2. The decision boundary is given by those points $\\mathbf{x}$ for which $\\mathbf{w}^T\\mathbf{x} = 0$. Show that the decision boundary is always orthogonal (at an angle of 90 degrees) to the weight vector. Recall the formula for the angle between two vectors for this.\n",
    "3. Suppose the input vector and weight vector have length 1. What input vector will lead to a maximal activity (maximal inner product)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 1</H3>\n",
    "\n",
    "Add your solution here.\n",
    "\n",
    "1. $w = (0.5, 0.5)^T$ with bias term $ b = 0.6 $ which gives $f(w^Tx - b)$ with the following truth table:\n",
    "\n",
    "$x_1$ | $x_2$ | $f(w^Tx - b)$\n",
    "--------- | -------- | -------\n",
    "1 | 1 | $f((0.5*1+0.5*1)-0.6) = f(0.4) = 1$\n",
    "1 | 0 | $f((0.5*1+0.5*0)-0.6) = f(-0.1) = 0$\n",
    "0 | 1 | $f((0.5*0+0.5*1)-0.6) = f(-0.1) = 0$\n",
    "0 | 0 | $f((0.5*0+0.5*0)-0.6) = f(-0.6) = 0$\n",
    "\n",
    "2. when $w^Tx = 0$, the cosine of the angle between them becomes $cos(\\theta) = \\frac{\\langle x,y\\rangle}{\\left\\lVert x \\right\\rVert \\left\\lVert y \\right\\rVert} = \\frac{0}{\\left\\lVert x \\right\\rVert \\left\\lVert y \\right\\rVert} = 0$ for all $x,w$.  \n",
    "\n",
    "3. The length of a vector $x$ is 1 when $\\sum_i x_i = 1$. Then the activation (inner product) between two vectors with length 1 can be 1 at maximum. When you plug this into the formula for the angle between two vectors,  $\\frac{\\langle x,y\\rangle}{\\left\\lVert x \\right\\rVert \\left\\lVert y \\right\\rVert} = \\frac{1}{1} = 1$, so the inner product is maximal when the cosine of the angle between the two vectors is 1, meaning when the angle corresponds to 0 degrees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (2 points)</H3>\n",
    "\n",
    "You will now derive the delta rule. Recall that the objective function is $E(\\mathbf{w}) = \\sum_n E^n(\\mathbf{w})$ where \n",
    "\\begin{equation}\n",
    "E^n(\\mathbf{w}) = \\frac{1}{2} (y^n - t^n)^2\n",
    "\\end{equation}\n",
    "is the objective for an example $(\\mathbf{x}^n,y^n)$. Note that $n$ here is an index indicating the $n$-th training sample.\n",
    "1. Consider an arbitrary weight $w_i$ in the weight vector $\\mathbf{w}$. From the sum rule we know that \n",
    "$\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial w_i} = \\sum_n\\frac{\\partial E^n(\\mathbf{w})}{\\partial w_i}.\n",
    "$\n",
    "Use the chain rule, which states that $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$, to derive that \n",
    "\\begin{equation}\n",
    "\\frac{\\partial E^n(\\mathbf{w})}{\\partial w_i} = (y^n - t^n) \\frac{\\partial y^n}{\\partial w_i}\\,.\n",
    "\\end{equation}\n",
    "2. Consider the latter part $\\frac{\\partial y^n}{\\partial w_i}$. Use $a^n = \\mathbf{w}^T\\mathbf{x}^n$ and $y^n = f(a^n)$ together with the chain rule to show that \n",
    "\\begin{equation}\n",
    "\\frac{\\partial y^n}{\\partial w_i} = \\frac{\\partial f(a^n)}{\\partial a^n} x_i^n.\n",
    "\\end{equation}\n",
    "\n",
    "If we put the previous two together, we have derived the partial derivative that is used in the delta rule:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial w_i} = \\sum_n (y^n - t^n) \\frac{\\partial f(a^n)}{\\partial a^n} x_i^n\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 2</H3>\n",
    "\n",
    "1. $\\frac{\\partial E^n(\\mathbf{w})}{\\partial w_i} =  \\frac{\\partial \\frac{1}{2} (y^n - t^n)^2}{\\partial w_i}$. $E^n(w)$ is a composite of two functions $E^n(z) = \\frac{1}{2} (z)^2$ and $ z(w) = y^n - t^n$ with $\\frac{\\partial E^n(z)}{\\partial z} = z$ and $\\frac{\\partial z(\\mathbf{w})}{\\partial w_i} = \\frac{\\partial y^n}{\\partial w_i}$. Then the chain rule gives us that $\\frac{\\partial E^n(\\mathbf{w})}{\\partial w_i} = \\frac{\\partial E^n(z)}{\\partial z} \\frac{\\partial z(\\mathbf{w})}{\\partial w_i} = (y^n - t^n) \\frac{\\partial y^n}{\\partial w_i}  $\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 3 (5 points)</H3>\n",
    "\n",
    "You will now implement your very own perceptron. Make sure you understand all steps you take: the single layer perceptron is the basis of the multilayer perceptron, and as such the basis of many supervised neural networks.\n",
    "\n",
    "Let's first define the problem we want to solve. We choose to learn the logical OR function defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This data is a representation of the logical OR, where X is the input and Y is the output. \n",
    "# The first row of X stands for the bias.\n",
    "X = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]],dtype='float32').transpose()\n",
    "T = np.array([0,1,1,1],dtype='float32').reshape([1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function *train_slp* which takes the following arguments\n",
    "* X: the data, $K \\times N$, with $K$ $=$ dimensionality of the data, $N$ $=$ number of patterns \n",
    "* Y: the labels, $1 \\times N$ (so only one output, either true or false)\n",
    "* eta: the learning rate\n",
    "* nepochs: the number of epochs it should run\n",
    "\n",
    "and returns\n",
    "\n",
    "* w: the weights\n",
    "\n",
    "Define the variables $N$ and $K$ in your function and initialize the weight to small random initial values.\n",
    "\n",
    "We now implement the perceptron algorithm. First we will set up the algorithm for one pattern, afterwards we will add a loop to handle multiple patterns. As you may remember from the lectures, the delta rule for the perceptron with a **linear activation function** states:\n",
    "\\begin{eqnarray*}\n",
    "y&=&f(\\mathbf{w}^T\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}\\\\\n",
    "\\Delta w_{i}&=&- \\eta (y^n-t^n)x^n_i\n",
    "\\end{eqnarray*}\n",
    "Make sure you know what this formula and the different variables mean, as you will now use them. \n",
    "\n",
    "First, calculate the weighted sum over the activations. This you can do using a for loop, or, more easily, using the matrix algebra you learned earlier: multiplying a row vector with a column vector gives you the inner product, which is just one number. To do so,\n",
    "select one, the first in this case, pattern from the dataset (recall we use zero-based indexing). Now multiply that with the weights. Next, we calculate the error term and calculate the change in weights in accordance with this error term, weighted by the input on that input node (the original pattern) and the learning rate. Then we update the weights.\n",
    "\n",
    "Once you got this weight update to work for one pattern, make it run over multiple epochs (training cycles) where in each training cycle you update all of the patterns. \n",
    "\n",
    "In general one does not want the patterns in sequence, but rather in a random order. To do so, you can make use of the function randperm. Implement this now and test if your function works. \n",
    "\n",
    "Note: You should take care that vectors have the right shape. Use *numpy.dot* to multiply vectors with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could run the network fully, but it won't tell you anything about the performance. To record the performance, make a vector called performance, in which you store the RMSE (root mean squared error) per epoch. In other words, the root of the mean of the squared error terms you calculated before. Run the algorithm and return not only the weights but also the performance. Plot the performance as a function of epoch number. Make sure that you use enough epochs and a proper learning rate to show convergence of the error. Play around with these parameters to create a nice looking plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a test function which allows you to test performance for different input patterns. Use the function to test whether the training went well. What do you conclude? Can you think of ways to obtain even more exact results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the algorithm again for the XOR problem and show that it does not produce an acceptable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your solution here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
