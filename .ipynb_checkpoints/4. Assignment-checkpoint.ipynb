{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work on multilayer neural networks. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 1 (3 points)</H3>\n",
    "\n",
    "1. Derive $\\frac{d f}{d a}$ for the sigmoid activation function:\n",
    "$\n",
    "f(a) = \\frac{1}{1 + \\exp(-a)}\n",
    "$\n",
    "and show that your derivation is equal to $f(a)(1-f(a))$.\n",
    "2. Show that the error $\\delta_j$  for an output unit of a multilayer network in case of the squared loss $E^n(\\mathbf{w}) = \\frac{1}{2} \\sum_k (y^n_k - t^n_k)^2$  and a sigmoid activation function is equal to:\n",
    "\\begin{equation}\n",
    "\\delta_j = (y^n_j - t^n_j) y^n_j (1 - y^n_j).\n",
    "\\end{equation}\n",
    "3. Multilayer neural networks assume that the activation function is both differentiable and non-linear. Explain why it must be differentiable and show mathematically that linear activation functions ($f(a) = a$) reduce multilayer neural networks to single layer neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 1</H3>\n",
    "\n",
    "1. $\\frac{d f}{d a} = \\frac{d \\frac{1}{1+\\exp(-a)}}{d a} = \\frac{d \\frac{1}{h}}{d h} \\frac{d (1+\\exp(-a))}{d a} = -\\frac{1}{h^2} \\exp(-a) = -\\frac{1}{(1+\\exp(-a))^2} \\exp(-a) = -\\frac{\\exp(-a)}{(1+exp(-a))^2} $\n",
    "\n",
    "The given function  $f(a)(1-f(a))$ can be rewritten as $\\frac{1}{1+\\exp(-a)} \\left ( \\frac{1+\\exp(-a)}{1+\\exp(-a)} - \\frac{1}{1+\\exp(-a)} \\right ) =  \\frac{1+\\exp(-a)}{(1+\\exp(-a))^2} - \\frac{1}{(1+\\exp(-a))^2} = \\frac{1}{1+\\exp(-a)} - \\frac{1}{(1+\\exp(-a))^2} = -\\frac{1- (1+\\exp(-a))}{(1+\\exp(-a))^2} = -\\frac{\\exp(-a)}{(1+\\exp(-a))^2} $ which is the same function as derived before. \n",
    " \n",
    "2. $\\delta_j =  \\frac{\\partial E^n }{\\partial a_j } = \\frac{\\partial E^n }{\\partial y^n_j } \\frac{\\partial y^n_j }{\\partial a_j } = \\frac{\\partial E^n }{\\partial y^n_j } \\frac{\\partial f(a_j) }{\\partial a_j } = \\frac{\\partial \\frac{1}{2} (y^n_j - t^n_j) }{\\partial y^n_j } \\frac{\\partial f(a_j) }{\\partial a_j } = (y^n_j - t^n_j) \\frac{\\partial f(a_j) }{\\partial a_j }  = (y^n_j - t^n_j) f(a)(1-f(a)) = (y^n_j - t^n_j)y^n_j(1-y^n_j)  $\n",
    "\n",
    "3. \n",
    "\n",
    "y = w3w2w1x = ux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (7 points)</H3>\n",
    "\n",
    "In the following exercise you will learn to implement the backpropagation algorithm. We provide most of the code. It is your job to implement the essential missing steps. We consider a problem where you need to classify which digit (0,1,...,9) each 20x20 pixel image, representing a handwritten digit, belongs to. We first read in the required training and test data from a matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "\n",
    "# read data from mat file\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('MLP_data.mat')\n",
    "    \n",
    "X_train = mat['X_training']\n",
    "X_test = mat['X_test']\n",
    "T_train = mat['T_training']\n",
    "T_test = mat['T_test']\n",
    "\n",
    "# take out all constant pixels since they are noninformative anyway\n",
    "idx = (np.std(X_train,1) != 0) & (np.std(X_test,1) != 0)\n",
    "X_train = X_train[idx,:]\n",
    "X_test = X_test[idx,:]\n",
    "\n",
    "# zscore data\n",
    "X_train = np.array(ss.zscore(X_train,1))\n",
    "X_test = np.array(ss.zscore(X_test,1))\n",
    "\n",
    "# add bias terms\n",
    "X_train = np.vstack([np.ones([1,X_train.shape[1]]),X_train])\n",
    "X_test = np.vstack([np.ones([1,X_test.shape[1]]),X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide an implementation of the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function; returns function value and gradient\n",
    "    \"\"\"\n",
    "\n",
    "    fx = 1.0 / (1 + np.exp(-x))\n",
    "    gradx = fx * (1 - fx)\n",
    "\n",
    "    return fx, gradx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function. For simplicity we use the squared error loss. Ideally however we would want to use the cross-entropy as a loss function but we ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(f_a_3,T):\n",
    "    \"\"\"\n",
    "    Computes squared error divided by number of trials\n",
    "    \n",
    "    Input:\n",
    "    f_a_3 : MLP output states\n",
    "    T   : noutput x ntrials targets\n",
    "\n",
    "    Output:\n",
    "    E_w        : squared error\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    ntrials = T.shape[1]\n",
    "\n",
    "    E_w = 1.0 / (2 * ntrials) * np.sum(np.sum((f_a_3 - T) ** 2))\n",
    "   \n",
    "    return E_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the forward pass for our MLP. Please fill in the missing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardprop(W_1, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs forward propagation\n",
    "    \n",
    "    Input:\n",
    "    W_1 : nhidden x ninput input-to-hidden weight matrix\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # You should now implement the forward propagation function. Your\n",
    "    # implementation should compute and return the outputs of the second and\n",
    "    # third layer units as well as their gradients.\n",
    "\n",
    "    # First, compute the inputs of the second layer units (i.e. a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    a_2 = np.dot(W_1, X)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_2, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the second\n",
    "    # layer units (i.e. f_a_2) and their gradients (i.e. grad_f_a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    f_a_2, grad_f_a_2 = sigmoid(a_2) \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Then, compute the inputs of the third layer units (i.e. a_3). Write your\n",
    "    # code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    a_3 = np.dot(W_2, f_a_2)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_3, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the third layer\n",
    "    # units (i.e. f_a_3) and their gradients (i.e. grad_f_a_3). Write your code\n",
    "    # below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    f_a_3, grad_f_a_3 = sigmoid(a_3)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    return f_a_2, f_a_3, grad_f_a_2, grad_f_a_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the backward pass for our MLP. Please fill in the missing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, T, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs backpropagation step\n",
    "    \n",
    "    Input:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    T   : noutput x ntrials targets\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    grad_E_w_1 : ntrials x 1 gradient of the error w.r.t W_1\n",
    "    grad_E_w_2 : ntrials x 1 gradient of the error w.r.t W_2\n",
    "    \"\"\"\n",
    "        \n",
    "    # You should now implement the back propagation function. Your\n",
    "    # implementation should compute and return the gradients of the error\n",
    "    # function.\n",
    "\n",
    "    # First, compute the errors of the second and third layer units (i.e.\n",
    "    # delta_2 and delta_3). Write you code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    delta_3 = (f_a_3 - T) * grad_f_a_3\n",
    "                   \n",
    "    delta_2 = grad_f_a_2 *(np.dot(W_2.T, delta_3))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed delta_2 and delta_3, use them to compute the\n",
    "    # gradients of the error function (i.e. grad_E_w_1 and grad_E_w_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    " \n",
    "    # Add your solution here.\n",
    "    grad_E_w_2 = np.dot(delta_3, f_a_2.T)\n",
    "    \n",
    "    grad_E_w_1 = np.dot(delta_2, X.T)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    return grad_E_w_1, grad_E_w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide the script with which you can test your MLP implementation. You should see that the error decreases for both the training and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 / 2000; Train error: [ 0.14037091]; Test error: [ 0.1575606]\n",
      "Iteration: 200 / 2000; Train error: [ 0.07859062]; Test error: [ 0.10517099]\n",
      "Iteration: 300 / 2000; Train error: [ 0.06004385]; Test error: [ 0.09220482]\n",
      "Iteration: 400 / 2000; Train error: [ 0.0501969]; Test error: [ 0.08735554]\n",
      "Iteration: 500 / 2000; Train error: [ 0.04364136]; Test error: [ 0.08561769]\n",
      "Iteration: 600 / 2000; Train error: [ 0.03890172]; Test error: [ 0.08511539]\n",
      "Iteration: 700 / 2000; Train error: [ 0.03518677]; Test error: [ 0.08498987]\n",
      "Iteration: 800 / 2000; Train error: [ 0.03218403]; Test error: [ 0.08499936]\n",
      "Iteration: 900 / 2000; Train error: [ 0.02969919]; Test error: [ 0.08514915]\n",
      "Iteration: 1000 / 2000; Train error: [ 0.02766153]; Test error: [ 0.08536537]\n",
      "Iteration: 1100 / 2000; Train error: [ 0.02593859]; Test error: [ 0.08559919]\n",
      "Iteration: 1200 / 2000; Train error: [ 0.02439936]; Test error: [ 0.08583459]\n",
      "Iteration: 1300 / 2000; Train error: [ 0.02307425]; Test error: [ 0.08593792]\n",
      "Iteration: 1400 / 2000; Train error: [ 0.02195449]; Test error: [ 0.08607612]\n",
      "Iteration: 1500 / 2000; Train error: [ 0.0209774]; Test error: [ 0.08624926]\n",
      "Iteration: 1600 / 2000; Train error: [ 0.02011438]; Test error: [ 0.08643409]\n",
      "Iteration: 1700 / 2000; Train error: [ 0.01934573]; Test error: [ 0.08661946]\n",
      "Iteration: 1800 / 2000; Train error: [ 0.01865782]; Test error: [ 0.08679847]\n",
      "Iteration: 1900 / 2000; Train error: [ 0.01803177]; Test error: [ 0.08696729]\n",
      "Iteration: 2000 / 2000; Train error: [ 0.01743907]; Test error: [ 0.08711169]\n"
     ]
    }
   ],
   "source": [
    "nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "ninput = X_train.shape[0]\n",
    "noutput = T_train.shape[0]\n",
    "nhidden = 15\n",
    "\n",
    "# initialize weights\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_1 = np.random.uniform(-r, r, [nhidden,ninput])\n",
    "\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_2 = np.random.uniform(-r, r, [noutput,nhidden])\n",
    "\n",
    "# keep track of errors\n",
    "train_error = np.zeros([nepochs+1,1])\n",
    "test_error = np.zeros([nepochs+1,1])\n",
    "\n",
    "# training\n",
    "for epoch in xrange(0,nepochs):\n",
    "\n",
    "    # First, use the forward propagation function that you have implemented\n",
    "    # (i.e. forwardprop) to compute the outputs of the second and third layer\n",
    "    # units (i.e. f_a_2 and f_a_3) as well as their gradients (i.e. grad_f_a_2\n",
    "    # and grad_f_a_3). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [f_a_2, f_a_3, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X_train)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # compute error\n",
    "    train_error[epoch] = error(f_a_3, T_train)\n",
    "    test_error[epoch] = error(forwardprop(W_1, W_2, X_test)[1], T_test)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "         print('Iteration: ' + str(epoch+1) + ' / ' + str(nepochs) + '; Train error: ' \n",
    "               + str(train_error[epoch])) + '; Test error: ' + str(test_error[epoch])\n",
    " \n",
    "    # Once you have computed f_a_2, f_a_3, grad_f_a_2 and grad_f_a_3, use them\n",
    "    # with the back propagation function that you have implemented (i.e.\n",
    "    # backprop) to compute the gradients of the error function (i.e. grad_E_w_1\n",
    "    # and grad_E_w_2). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [grad_E_w_1, grad_E_w_2] = backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, T_train, W_2, X_train)\n",
    "    # -------------------------------------------------------------------------\n",
    "             \n",
    "    W_1 = W_1 - learning_rate * grad_E_w_1                                 \n",
    "    W_2 = W_2 - learning_rate * grad_E_w_2                                                                            \n",
    "    \n",
    "# get error after the last update\n",
    "train_error[-1] = error(forwardprop(W_1, W_2, X_train)[1], T_train)\n",
    "test_error[-1] = error(forwardprop(W_1, W_2, X_test)[1], T_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of the decrease in training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd046320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFqlJREFUeJzt3X2QHPV95/H3V/ugRyT0sGChZ7DAlsEQs8GcD2xcztlA\nHJSHqxTEZTs+Jzoqds6pq9SZlOscV/kvnyuplMs2KuJQJHcxJBdjm6vCwYnjmFxxGAQIkIwFQmAk\nIfSAQBISaFnt9/7YXhgtmu1Z7ezM9uz7VbWl6e7fdH/VM/uZ3l//uicyE0lSZ5nR7gIkSc1nuEtS\nBzLcJakDGe6S1IEMd0nqQIa7JHUgw12SOpDhLkkdyHCXpA7U3a4NL1myJFevXt2uzUtSJT300EMH\nMrOvrF3bwn316tVs2rSpXZuXpEqKiF800s5uGUnqQIa7JHUgw12SOpDhLkkdyHCXpA5kuEtSBzLc\nJakDVS7cn9x7hD//4TYOvHK83aVI0pRVuXB/au8rfO1ftnPw6EC7S5GkKaty4S5JKlfZcM9sdwWS\nNHVVLtwj2l2BJE19lQt3SVI5w12SOlBlwz2x012S6qlcuNvlLknlSsM9Im6NiH0RsaXO8o9FxGMR\n8XhE3BcRFze/TEnSeDRy5H4bcPUYy58BPpCZFwFfBm5pQl2lHAopSfWVfs1eZt4bEavHWH5fzeT9\nwPKJl1WfQyElqVyz+9w/DfygyeuUJI1T074gOyI+yHC4XzFGmw3ABoCVK1dOaHt2y0hSfU05co+I\ndwPfAtZn5ov12mXmLZnZn5n9fX19p7u103yeJE0fEw73iFgJ3Al8PDOfnHhJkqSJKu2WiYjbgauA\nJRGxC/hToAcgMzcCXwQWA9+M4bOdg5nZP1kFS5LKNTJa5oaS5b8H/F7TKmqQV6hKUn3Vu0LVLndJ\nKlW5cJcklatsuDsUUpLqq1y42ysjSeUqF+6SpHKGuyR1oMqFezhcRpJKVS7cJUnlDHdJ6kCVDXeH\nQkpSfZULd3vcJalc5cJdklSusuHujcMkqb7KhbsjISWpXOXCXZJUrrLh7mgZSaqvcuFut4wklatc\nuEuSyhnuktSBKhvudrlLUn2VC/fwGlVJKlW5cJcklatsuKdjISWpruqFu70yklSqNNwj4taI2BcR\nW+osj4j4WkRsj4jHIuI9zS9TkjQejRy53wZcPcbya4C1xc8G4OaJl1XOThlJqq803DPzXuDgGE3W\nA3+Tw+4HzoyIpc0qcDR7ZSSpXDP63JcBO2umdxXzJElt0tITqhGxISI2RcSm/fv3t3LTkjStNCPc\ndwMraqaXF/PeIjNvycz+zOzv6+ub0EYdCSlJ9TUj3O8CPlGMmrkcOJSZe5qw3lMKbwspSaW6yxpE\nxO3AVcCSiNgF/CnQA5CZG4G7gWuB7cAx4FOTVawkqTGl4Z6ZN5QsT+AzTauoYfbLSFI9lbtC1U4Z\nSSpXuXCXJJWrbLg7WkaS6qtcuDtYRpLKVS7cJUnlDHdJ6kCVDXe73CWpvsqFu9+hKknlKhfukqRy\nlQ13h0JKUn2VC3eHQkpSucqFuySpXGXDPe2XkaS6Khfu9spIUrnKhbskqZzhLkkdqLLhbo+7JNVX\nvXC3012SSlUv3CVJpSob7o6ElKT6Khfu3jhMkspVLtwlSeUqG+7peBlJqqty4e6NwySpXOXCXZJU\nrqFwj4irI2JbRGyPiJtOsXxBRPyfiHg0IrZGxKeaX6okqVGl4R4RXcA3gGuAdcANEbFuVLPPAD/L\nzIuBq4A/i4jeJtd6MrvcJamuRo7cLwO2Z+aOzBwA7gDWj2qTwBkREcA84CAw2NRKC3a5S1K5RsJ9\nGbCzZnpXMa/W14F3As8DjwOfy8yh0SuKiA0RsSkiNu3fv/80S5YklWnWCdWPAJuBc4BLgK9HxPzR\njTLzlszsz8z+vr6+CW3QXhlJqq+RcN8NrKiZXl7Mq/Up4M4cth14BnhHc0o8WTgWUpJKNRLuDwJr\nI2JNcZL0euCuUW2eAz4EEBFnAxcAO5pZqCSpcd1lDTJzMCI+C9wDdAG3ZubWiLixWL4R+DJwW0Q8\nzvA5z89n5oFJrNsbh0nSGErDHSAz7wbuHjVvY83j54EPN7e0U7NXRpLKeYWqJHUgw12SOlBlw927\nQkpSfZULd7vcJalc5cJdklSusuHuUEhJqq9y4e5QSEkqV7lwlySVq2y42ysjSfVVMNztl5GkMhUM\nd0lSGcNdkjpQZcM9HQspSXVVLtwdCilJ5SoX7pKkcpUNdztlJKm+yoW7vTKSVK5y4S5JKlfdcLdf\nRpLqqm64S5Lqqly4h2MhJalU5cJdklSusuHud6hKUn2VC3c7ZSSpXEPhHhFXR8S2iNgeETfVaXNV\nRGyOiK0R8ZPmlilJGo/usgYR0QV8A/gPwC7gwYi4KzN/VtPmTOCbwNWZ+VxEnDVZBY/wvmGSVF8j\nR+6XAdszc0dmDgB3AOtHtfkd4M7MfA4gM/c1t8w3OVhGkso1Eu7LgJ0107uKebXOBxZGxL9GxEMR\n8YlmFShJGr/SbplxrOdS4EPAbOD/RcT9mflkbaOI2ABsAFi5cuWENmi3jCTV18iR+25gRc308mJe\nrV3APZl5NDMPAPcCF49eUWbekpn9mdnf19d3ujVLkko0Eu4PAmsjYk1E9ALXA3eNavN94IqI6I6I\nOcB7gSeaW+qwcDCkJJUq7ZbJzMGI+CxwD9AF3JqZWyPixmL5xsx8IiL+EXgMGAK+lZlbJrNwSVJ9\nDfW5Z+bdwN2j5m0cNf1V4KvNK62kplZtSJIqqHpXqNorI0mlKhfukqRylQ33dCykJNVV2XCXJNVn\nuEtSB6psuNspI0n1VTbcJUn1VS7cHQopSeUqF+6SpHKVDXdHQkpSfZULd28cJknlKhfukqRylQv3\nkROqXqEqSfVVLtxn9XQB8NrgiTZXIklTV+XCfW7vcLgfPW64S1I9lQv3OTOHb0F/bGCwzZVI0tRV\nuXCf3eORuySVqVy4d80IZvd0eeQuSWOoXLgDzJ3ZzdEBj9wlqZ5KhvuSeb08//Kr7S5DkqasSob7\nuqXz2fr8Yce6S1IdlQz3/tWL2H/kOE/vf6XdpUjSlFTJcL9y7RIA7n3yQJsrkaSpqZLhvmLRHM5d\nMpd7n9rf7lIkaUqqZLgDvP/8Pu7f8SLHvQ2BJL1FQ+EeEVdHxLaI2B4RN43R7pcjYjAi/mPzSjy1\nK9cu4bXXh9j07EuTvSlJqpzScI+ILuAbwDXAOuCGiFhXp91XgB82u8hTufzcxfR0hV0zknQKjRy5\nXwZsz8wdmTkA3AGsP0W7PwS+A+xrYn11zZ3ZzbpzFvDozpdbsTlJqpRGwn0ZsLNmelcx7w0RsQz4\nDeDmsVYUERsiYlNEbNq/f+JH3Bctm8/W3YcZGnK8uyTVatYJ1b8APp+ZQ2M1ysxbMrM/M/v7+vom\nvNELz1nAkeODPHfw2ITXJUmdpLuBNruBFTXTy4t5tfqBO2L4a5KWANdGxGBmfq8pVdZx4bIFAGx5\n/hCrl8ydzE1JUqU0cuT+ILA2ItZERC9wPXBXbYPMXJOZqzNzNfAPwB9MdrADnH/2GfR2zWDL7sOT\nvSlJqpTSI/fMHIyIzwL3AF3ArZm5NSJuLJZvnOQa6+rtnsEFbzuDLbsPtasESZqSGumWITPvBu4e\nNe+UoZ6Zvzvxshp34bL5/GDLC2QmMfLt2ZI0zVX2CtURFy5bwMvHXmfXS94CWJJGVD/czxk+qbr1\nebtmJGlE5cP9gredQdeM8KSqJNWofLjP6uli7VnzeNyTqpL0hsqHO8BFyxawZfchv5lJkgodEe4X\nLlvAi0cH2Hv4eLtLkaQpoUPCfT6AXTOSVOiIcH/n0vnMCLyYSZIKHRHuc3q7Oa9vnsMhJanQEeEO\nw/3udstI0rCOCfd3nTOfvYePs+/wa+0uRZLarmPCvX/1IgAeePZgmyuRpPbrmHB/1znzmdPbxQPP\nGO6S1DHh3tM1g0tXLeSnOwx3SeqYcAd475pFbNt7hJeODrS7FElqq84K93MXA/DTZ15scyWS1F4d\nFe6XrDiTM2Z18+Of7293KZLUVh0V7j1dM3j/+X38y7Z9DA15EzFJ01dHhTvAh95xFvuPHGeLV6tK\nmsY6LtyvuuAsIuBHT+xrdymS1DYdF+6L5vbSv2oh/7jlhXaXIklt03HhDnDdxeewbe8RntjjV+9J\nmp46Mtx/9d3n0D0j+N4ju9tdiiS1RUeG+6K5vbz//D7uevR5R81ImpYaCveIuDoitkXE9oi46RTL\nPxYRj0XE4xFxX0Rc3PxSx+fXf2kZew69xn1Pe0GTpOmnNNwjogv4BnANsA64ISLWjWr2DPCBzLwI\n+DJwS7MLHa8PrzubhXN6+F/3/6LdpUhSyzVy5H4ZsD0zd2TmAHAHsL62QWbel5kvFZP3A8ubW+b4\nzerp4rf7V/BPT+zlhUPe413S9NJIuC8DdtZM7yrm1fNp4AcTKapZfue9KxnK5PYHnmt3KZLUUk09\noRoRH2Q43D9fZ/mGiNgUEZv275/8+7+sWjyXD5zfx7cfeI7XXj8x6duTpKmikXDfDayomV5ezDtJ\nRLwb+BawPjNPeRYzM2/JzP7M7O/r6zudesft9688l/1HjvOdh3e1ZHuSNBU0Eu4PAmsjYk1E9ALX\nA3fVNoiIlcCdwMcz88nml3n63nfeYi5ZcSYbf/I0gyeG2l2OJLVEabhn5iDwWeAe4Ang7zNza0Tc\nGBE3Fs2+CCwGvhkRmyNi06RVPE4RwWc++HZ2HnyVO72oSdI0EZntucinv78/N21qzWfA0FDymzff\nx+6XX+XHf3wV82Z2t2S7ktRsEfFQZvaXtevIK1RHmzEj+NJ172L/keN87UdPtbscSZp00yLcYfhb\nmq7/5RV86992cP8Or1qV1NmmTbgD/PePrmPV4rl87o5HeP7lV9tdjiRNmmkV7nNndvPNj72HY8dP\n8PG/+ikvvnK83SVJ0qSYVuEO8M6l8/nLT/az66VX+a2b7+PZA0fbXZIkNd20C3eAy89dzLd//3IO\nvzbIdV//v3x/s0MkJXWWaRnuAJeuWsj3/uDf8/az5vG5Ozbz6dse5Km9R9pdliQ1xbQNd4CVi+fw\n9//53/En17yDB545yEf+4l7+y+2P8MhzL5U/WZKmsGlxEVMjDh4d4OZ/3c4dD+zkyPFB1i2dz69d\nfA4fffdSViya0+7yJAlo/CImw32UV44P8t2Hd/Gdh3ezeefLAFxw9hm87+2LueLtS+hfvYgFs3va\nXKWk6cpwb4KdB49x9+N7+LenDvDgswc5Pjh847FVi+dw4bIFXLRsAeefPY81S+axfOFserqmdS+X\npBYw3JvstddP8PBzL/HIcy+zZfchHt99iF0vvXkhVPeMYMWiOaxaPIelC2Zx9vxZvG3+LM5eMIul\nC2axeO5MFszuobfbDwBJp6/RcPcOWg2a1dPF+85bwvvOW/LGvJePDfD0/qM8c+Aozxx4hWcPHOPZ\nF4+yZfchDrwycMr1zJvZzYLZPZw5p4eFc3pZMKeHeb3dzO7tYk5vF3NndjO7Z/jxnJndzOnpYnZv\nF73dM+jpmkFPV9DbVTzuHjVdLI+IVu0WVdDIAd3IcV2Onv/G9Mjyk9tTsrxsfZzm85Ic9fzyOkb/\nXydce932ddZXZ/6SeTN524JZTCbDfQLOnNPLpat6uXTVwrcsGxgcYt+R19h7+DVeOHScg0eP8/Kx\n13n51dd56dgAh4rHz+95lWPHT3B0YJBjAyc4MTTxv6QiYEYEM2L4lscz3piOk5YNT9cuL9rPgKD8\nA6LsM6RsDWUfQo18RI03mN76S97g80Ytr/9LXmd9o+a/NaCy4RpONxw1ddz4gfO46Zp3TOo2DPdJ\n0ts9g+UL57B8YeMjbTKTgRNDvDpwgmMDJzhWBP6xgRMMnkhePzHEwImh4X8Hi39PJK8Xj0emM5NM\nGMpkKIfXO/J46KRlNcuHOKlNI7WOubz0+SXLG9j+yIfDyIfAyGfFm9MnL+eN5VGnfZ3lo1bQ8PNG\n1UED7d/cZpNqH7XRxmsef+1jLX/z+Q2+Zs2qn6jZdyU1nHbtY9dwqvfN6iVzmWyG+xQSEczs7mJm\ndxdnOvpS0gR4dk+SOpDhLkkdyHCXpA5kuEtSBzLcJakDGe6S1IEMd0nqQIa7JHWgtt04LCL2A784\nzacvAQ40sZxmmap1wdStzbrGx7rGpxPrWpWZfWWN2hbuExERmxq5K1qrTdW6YOrWZl3jY13jM53r\nsltGkjqQ4S5JHaiq4X5LuwuoY6rWBVO3NusaH+san2lbVyX73CVJY6vqkbskaQyVC/eIuDoitkXE\n9oi4qcXbXhERP46In0XE1oj4XDH/SxGxOyI2Fz/X1jznT4pat0XERyaxtmcj4vFi+5uKeYsi4p8i\n4qni34U17Se9roi4oGafbI6IwxHxR+3YXxFxa0Tsi4gtNfPGvX8i4tJiP2+PiK/FBL/TsE5dX42I\nn0fEYxHx3Yg4s5i/OiJerdlvG1tc17hftxbV9Xc1NT0bEZuL+a3cX/WyoX3vseFv7anGD9AFPA2c\nC/QCjwLrWrj9pcB7isdnAE8C64AvAX98ivbrihpnAmuK2rsmqbZngSWj5v0P4Kbi8U3AV1pd16jX\n7gVgVTv2F/B+4D3AlonsH+AB4HKGv1DnB8A1k1DXh4Hu4vFXaupaXdtu1HpaUde4X7dW1DVq+Z8B\nX2zD/qqXDW17j1XtyP0yYHtm7sjMAeAOYH2rNp6ZezLz4eLxEeAJYNkYT1kP3JGZxzPzGWA7w/+H\nVlkP/HXx+K+BX29jXR8Cns7MsS5cm7S6MvNe4OApttfw/omIpcD8zLw/h38L/6bmOU2rKzN/mJmD\nxeT9wPKx1tGqusbQ1v01ojjC/W3g9rHWMUl11cuGtr3Hqhbuy4CdNdO7GDtcJ01ErAZ+CfhpMesP\niz+jb63506uV9SbwzxHxUERsKOadnZl7iscvAGe3oa4R13PyL1279xeMf/8sKx63qj6A/8Tw0duI\nNUUXw08i4spiXivrGs/r1ur9dSWwNzOfqpnX8v01Khva9h6rWrhPCRExD/gO8EeZeRi4meGuokuA\nPQz/adhqV2TmJcA1wGci4v21C4ujgLYMjYqIXuA64H8Xs6bC/jpJO/dPPRHxBWAQ+Nti1h5gZfE6\n/1fg2xExv4UlTbnXbZQbOPkAouX76xTZ8IZWv8eqFu67gRU108uLeS0TET0Mv3h/m5l3AmTm3sw8\nkZlDwF/yZldCy+rNzN3Fv/uA7xY17C3+zBv5U3Rfq+sqXAM8nJl7ixrbvr8K490/uzm5i2TS6ouI\n3wU+CnysCAWKP+FfLB4/xHA/7fmtqus0XrdW7q9u4DeBv6upt6X761TZQBvfY1UL9weBtRGxpjga\nvB64q1UbL/r0/gp4IjP/vGb+0ppmvwGMnMm/C7g+ImZGxBpgLcMnS5pd19yIOGPkMcMn5LYU2/9k\n0eyTwPdbWVeNk46o2r2/aoxr/xR/Xh+OiMuL98Inap7TNBFxNfDfgOsy81jN/L6I6Coen1vUtaOF\ndY3rdWtVXYVfAX6emW90abRyf9XLBtr5HpvIGeJ2/ADXMnwm+mngCy3e9hUM/1n1GLC5+LkW+J/A\n48X8u4ClNc/5QlHrNiZ4Rn6Mus5l+Mz7o8DWkf0CLAZ+BDwF/DOwqJV1FduZC7wILKiZ1/L9xfCH\nyx7gdYb7MT99OvsH6Gc41J4Gvk5xIWCT69rOcH/syHtsY9H2t4rXdzPwMPBrLa5r3K9bK+oq5t8G\n3DiqbSv3V71saNt7zCtUJakDVa1bRpLUAMNdkjqQ4S5JHchwl6QOZLhLUgcy3CWpAxnuktSBDHdJ\n6kD/H7GWABA1tvq/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcecabe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plot here\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_error)\n",
    "plt.title('Error over epochs training set')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_error)\n",
    "plt.title('Error over epochs training set')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
