{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will work on multilayer neural networks. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 1 (3 points)</H3>\n",
    "\n",
    "1. Derive $\\frac{d f}{d a}$ for the sigmoid activation function:\n",
    "$\n",
    "f(a) = \\frac{1}{1 + \\exp(-a)}\n",
    "$\n",
    "and show that your derivation is equal to $f(a)(1-f(a))$.\n",
    "2. Show that the error $\\delta_j$  for an output unit of a multilayer network in case of the squared loss $E^n(\\mathbf{w}) = \\frac{1}{2} \\sum_k (y^n_k - t^n_k)^2$  and a sigmoid activation function is equal to:\n",
    "\\begin{equation}\n",
    "\\delta_j = (y^n_j - t^n_j) y^n_j (1 - y^n_j).\n",
    "\\end{equation}\n",
    "3. Multilayer neural networks assume that the activation function is both differentiable and non-linear. Explain why it must be differentiable and show mathematically that linear activation functions ($f(a) = a$) reduce multilayer neural networks to single layer neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 1</H3>\n",
    "\n",
    "1. $\\frac{d f}{d a} = \\frac{d \\frac{1}{1+\\exp(-a)}}{d a} = \\frac{d \\frac{1}{h}}{d h} \\frac{d (1+\\exp(-a))}{d a} = -\\frac{1}{h^2} \\exp(-a) = -\\frac{1}{(1+\\exp(-a))^2} \\exp(-a) = -\\frac{\\exp(-a)}{(1+exp(-a))^2} $\n",
    "\n",
    "The given function  $f(a)(1-f(a))$ can be rewritten as $\\frac{1}{1+\\exp(-a)} \\left ( \\frac{1+\\exp(-a)}{1+\\exp(-a)} - \\frac{1}{1+\\exp(-a)} \\right ) =  \\frac{1+\\exp(-a)}{(1+\\exp(-a))^2} - \\frac{1}{(1+\\exp(-a))^2} = \\frac{1}{1+\\exp(-a)} - \\frac{1}{(1+\\exp(-a))^2} = -\\frac{1- (1+\\exp(-a))}{(1+\\exp(-a))^2} = -\\frac{\\exp(-a)}{(1+\\exp(-a))^2} $ which is the same function as derived before. \n",
    " \n",
    "2. $\\delta_j =  \\frac{\\partial E^n }{\\partial a_j } = \\frac{\\partial E^n }{\\partial y^n_j } \\frac{\\partial y^n_j }{\\partial a_j } = \\frac{\\partial E^n }{\\partial y^n_j } \\frac{\\partial f(a_j) }{\\partial a_j } = \\frac{\\partial \\frac{1}{2} (y^n_j - t^n_j) }{\\partial y^n_j } \\frac{\\partial f(a_j) }{\\partial a_j } = (y^n_j - t^n_j) \\frac{\\partial f(a_j) }{\\partial a_j }  = (y^n_j - t^n_j) f(a)(1-f(a)) = (y^n_j - t^n_j)y^n_j(1-y^n_j)  $\n",
    "\n",
    "3. \n",
    "\n",
    "y = w3w2w1x = ux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (7 points)</H3>\n",
    "\n",
    "In the following exercise you will learn to implement the backpropagation algorithm. We provide most of the code. It is your job to implement the essential missing steps. We consider a problem where you need to classify which digit (0,1,...,9) each 20x20 pixel image, representing a handwritten digit, belongs to. We first read in the required training and test data from a matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "\n",
    "# read data from mat file\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('MLP_data.mat')\n",
    "    \n",
    "X_train = mat['X_training']\n",
    "X_test = mat['X_test']\n",
    "T_train = mat['T_training']\n",
    "T_test = mat['T_test']\n",
    "\n",
    "# take out all constant pixels since they are noninformative anyway\n",
    "idx = (np.std(X_train,1) != 0) & (np.std(X_test,1) != 0)\n",
    "X_train = X_train[idx,:]\n",
    "X_test = X_test[idx,:]\n",
    "\n",
    "# zscore data\n",
    "X_train = np.array(ss.zscore(X_train,1))\n",
    "X_test = np.array(ss.zscore(X_test,1))\n",
    "\n",
    "# add bias terms\n",
    "X_train = np.vstack([np.ones([1,X_train.shape[1]]),X_train])\n",
    "X_test = np.vstack([np.ones([1,X_test.shape[1]]),X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide an implementation of the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function; returns function value and gradient\n",
    "    \"\"\"\n",
    "\n",
    "    fx = 1.0 / (1 + np.exp(-x))\n",
    "    gradx = fx * (1 - fx)\n",
    "\n",
    "    return fx, gradx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function. For simplicity we use the squared error loss. Ideally however we would want to use the cross-entropy as a loss function but we ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(f_a_3,T):\n",
    "    \"\"\"\n",
    "    Computes squared error divided by number of trials\n",
    "    \n",
    "    Input:\n",
    "    f_a_3 : MLP output states\n",
    "    T   : noutput x ntrials targets\n",
    "\n",
    "    Output:\n",
    "    E_w        : squared error\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    ntrials = T.shape[1]\n",
    "\n",
    "    E_w = 1.0 / (2 * ntrials) * np.sum(np.sum((f_a_3 - T) ** 2))\n",
    "   \n",
    "    return E_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the forward pass for our MLP. Please fill in the missing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardprop(W_1, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs forward propagation\n",
    "    \n",
    "    Input:\n",
    "    W_1 : nhidden x ninput input-to-hidden weight matrix\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # You should now implement the forward propagation function. Your\n",
    "    # implementation should compute and return the outputs of the second and\n",
    "    # third layer units as well as their gradients.\n",
    "\n",
    "    # First, compute the inputs of the second layer units (i.e. a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Add your solution here.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_2, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the second\n",
    "    # layer units (i.e. f_a_2) and their gradients (i.e. grad_f_a_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Add your solution here.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Then, compute the inputs of the third layer units (i.e. a_3). Write your\n",
    "    # code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Add your solution here.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed a_3, use it with the sigmoid function that you\n",
    "    # have implemented (i.e. sigmoid) to compute the outputs of the third layer\n",
    "    # units (i.e. f_a_3) and their gradients (i.e. grad_f_a_3). Write your code\n",
    "    # below:\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Add your solution here.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    return f_a_2, f_a_3, grad_f_a_2, grad_f_a_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the backward pass for our MLP. Please fill in the missing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, T, W_2, X):\n",
    "    \"\"\"\n",
    "    Performs backpropagation step\n",
    "    \n",
    "    Input:\n",
    "    f_a_2 : MLP hidden unit states\n",
    "    f_a_3 : MLP output states\n",
    "    grad_f_a_2 : gradient of the hidden unit activation function\n",
    "    grad_f_a_3 : gradient of the output unit activation function\n",
    "    T   : noutput x ntrials targets\n",
    "    W_2 : noutput x nhidden hidden-to-output weight matrix\n",
    "    X   : ninput x ntrials input data\n",
    "    \n",
    "    Output:\n",
    "    grad_E_w_1 : ntrials x 1 gradient of the error w.r.t W_1\n",
    "    grad_E_w_2 : ntrials x 1 gradient of the error w.r.t W_2\n",
    "    \"\"\"\n",
    "        \n",
    "    # You should now implement the back propagation function. Your\n",
    "    # implementation should compute and return the gradients of the error\n",
    "    # function.\n",
    "\n",
    "    # First, compute the errors of the second and third layer units (i.e.\n",
    "    # delta_2 and delta_3). Write you code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Add your solution here.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Once you have computed delta_2 and delta_3, use them to compute the\n",
    "    # gradients of the error function (i.e. grad_E_w_1 and grad_E_w_2). Write\n",
    "    # your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    " \n",
    "    # Add your solution here.\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    return grad_E_w_1, grad_E_w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide the script with which you can test your MLP implementation. You should see that the error decreases for both the training and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "ninput = X_train.shape[0]\n",
    "noutput = T_train.shape[0]\n",
    "nhidden = 15\n",
    "\n",
    "# initialize weights\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_1 = np.random.uniform(-r, r, [nhidden,ninput])\n",
    "\n",
    "r = np.sqrt(6)/np.sqrt(nhidden+ninput)\n",
    "W_2 = np.random.uniform(-r, r, [noutput,nhidden])\n",
    "\n",
    "# keep track of errors\n",
    "train_error = np.zeros([nepochs+1,1])\n",
    "test_error = np.zeros([nepochs+1,1])\n",
    "\n",
    "# training\n",
    "for epoch in xrange(0,nepochs):\n",
    "\n",
    "    # First, use the forward propagation function that you have implemented\n",
    "    # (i.e. forwardprop) to compute the outputs of the second and third layer\n",
    "    # units (i.e. f_a_2 and f_a_3) as well as their gradients (i.e. grad_f_a_2\n",
    "    # and grad_f_a_3). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [f_a_2, f_a_3, grad_f_a_2, grad_f_a_3] = forwardprop(W_1, W_2, X_train)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # compute error\n",
    "    train_error[epoch] = error(f_a_3, T_train)\n",
    "    test_error[epoch] = error(forwardprop(W_1, W_2, X_test)[1], T_test)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "         print('Iteration: ' + str(epoch+1) + ' / ' + str(nepochs) + '; Train error: ' \n",
    "               + str(train_error[epoch])) + '; Test error: ' + str(test_error[epoch])\n",
    " \n",
    "    # Once you have computed f_a_2, f_a_3, grad_f_a_2 and grad_f_a_3, use them\n",
    "    # with the back propagation function that you have implemented (i.e.\n",
    "    # backprop) to compute the gradients of the error function (i.e. grad_E_w_1\n",
    "    # and grad_E_w_2). Write your code below:\n",
    "    # -------------------------------------------------------------------------\n",
    "    [grad_E_w_1, grad_E_w_2] = backprop(f_a_2, f_a_3, grad_f_a_2, grad_f_a_3, T_train, W_2, X_train)\n",
    "    # -------------------------------------------------------------------------\n",
    "             \n",
    "    W_1 = W_1 - learning_rate * grad_E_w_1                                 \n",
    "    W_2 = W_2 - learning_rate * grad_E_w_2                                                                            \n",
    "    \n",
    "# get error after the last update\n",
    "train_error[-1] = error(forwardprop(W_1, W_2, X_train)[1], T_train)\n",
    "test_error[-1] = error(forwardprop(W_1, W_2, X_test)[1], T_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of the decrease in training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create plot here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
