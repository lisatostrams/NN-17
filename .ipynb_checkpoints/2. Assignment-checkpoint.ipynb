{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this first assignment you will practice with some of the mathematical techniques that are required in order to develop neural networks. You will also implement some simple functions to build an intuition for what neural networks are. Always show how you arrived at your answer. Hand in your assignment by adding the solutions to this notebook file. Consult a page on markdown language and a page on LaTeX in order to learn how to write down your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>Exercise 1 (4 points)</H3>\n",
    "We start with some preparatory math exercises. Let $\\mathbf{x} = (1,2)^T$, $\\mathbf{y} = (-1,1)^T$ and $\\mathbf{Z} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "$.\n",
    "1. What is the length (norm) of the vector $\\mathbf{x}$\n",
    "2. How much is $\\mathbf{x}^T\\mathbf{y}$\n",
    "3. How much is $\\mathbf{Z}\\mathbf{x}$?\n",
    "4. What is the angle between $\\mathbf{x}$ and $\\mathbf{y}$ in degrees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 1</H3>\n",
    "\n",
    "1. $norm(x) = \\sqrt(1^2 + 2^2) = \\sqrt 5$\n",
    "2. $x^Ty = 1*-1 + 2*1 = 1 $\n",
    "3. $Zx = (1*1 + 2*2, 3*1 + 4*2)^T = (5,11)^T$\n",
    "4. $cos(\\theta) = \\frac{\\langle x,y\\rangle}{\\left\\lVert x \\right\\rVert \\left\\lVert y \\right\\rVert} = \\frac{1}{\\sqrt 5 \\sqrt 2} = 0.3162$ which corresponds to 71.5651 degrees (rad). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 2 (2 points)</H3>\n",
    "\n",
    "You will now practice with partial derivatives. \n",
    "\n",
    "1. Let $\\mathbf{x} = (x_1,\\ldots,x_n)^T$ and $f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$. Write down the expression for the partial derivative $\\frac{\\partial f}{\\partial x_i}$. \n",
    "Hint: rewrite the function $f(\\mathbf{x})$ in terms of scalars instead of vectors. Consult previous courses and background material online to brush up on partial derivatives. Note that for a partial derivative w.r.t. $x_i$ we assume all the other variables to be constants. Tip: often we can directly compute (partial) derivatives of matrix expressions. Consult http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274 for a useful reference on matrix manipulation. \n",
    "2. Often, we need to compute the gradient of a particular function. Given a function $f(x_1,\\ldots,x_n)$, the gradient is just a collection of partial derivatives:\n",
    "\\begin{equation*}\n",
    "\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\ldots,\\frac{\\partial f}{\\partial x_n}\\right) \\,.\n",
    "\\end{equation*}\n",
    "Consider the function $f(x,y) = - (\\cos^2 x + \\cos^2 y)^2$. Derive the gradient $\\nabla f = \\left(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 2</H3>\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 3 (2 points)</H3>\n",
    "\n",
    "Let's practice with some basic probability theory. \n",
    "\n",
    "1. Write down the logarithm of a univariate Gaussian $\\mathcal{N}(x ; \\mu, \\sigma^2)$ with mean $\\mu$ and variance $\\sigma^2$ and simplify where possible\n",
    "2. Compute $\\frac{\\partial \\log \\mathcal{N}(x ; \\mu, \\sigma^2)}{\\partial \\mu}$ and simplify where possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Solution 3</H3>\n",
    "\n",
    "Add your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Exercise 4 (2 points)</H3>\n",
    "\n",
    "Now write a function $f$ which computes $\\frac{\\partial \\log \\mathcal{N}(x ; \\mu, \\sigma^2)}{\\partial \\mu}$ for arbitrary values of $x$, $\\mu$ and $\\sigma^2$. Use this function to plot $f(x,\\mu, \\sigma^2)$ as you keep $x = 1$, $\\sigma^2 = 1$ and vary $\\mu$. That is, the x-axis of the plot should show $\\mu$ and the y-axis should show $f(x,\\mu, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that if we take tiny steps in the direction of the partial derivative then the derivative will be zero at $\\mu=1$. Hence, we can find the optimal value of $\\mu$ like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large and complex neural networks, the workhorse typically remains some form of updating based on derivatives. However, computing the derivatives can be cumbersome for large models. That's why modern neural network packages (TensorFlow, Chainer, Theano, etc.) use [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
